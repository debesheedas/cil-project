{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentHashtags import *\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = '/kaggle/input/twitter-dataset/train_pos.txt'\n",
    "file2 = '/kaggle/input/twitter-dataset/train_neg.txt'\n",
    "\n",
    "df = pd.read_fwf(file1)\n",
    "df.to_csv('train_pos.csv', index=False)\n",
    "\n",
    "df = pd.read_fwf(file2)\n",
    "df.to_csv('train_neg.csv', index=False)\n",
    "\n",
    "import csv\n",
    "with open('train_neg.csv', 'r') as file:\n",
    "        # Read the existing data from the CSV file\n",
    "        reader = csv.reader(file)\n",
    "        data = list(reader)\n",
    "\n",
    "    # Insert column with 1s at the beginning of each row\n",
    "for row in data:\n",
    "        row.insert(0, '1')\n",
    "\n",
    "with open('train_neg_merged.csv', 'w', newline='') as file:\n",
    "        # Write the modified data back to the CSV file\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data)\n",
    "        \n",
    "        \n",
    "with open('train_pos.csv', 'r') as file:\n",
    "        # Read the existing data from the CSV file\n",
    "        reader = csv.reader(file)\n",
    "        data = list(reader)\n",
    "\n",
    "    # Insert column with 1s at the beginning of each row\n",
    "for row in data:\n",
    "        row.insert(0, '0')\n",
    "\n",
    "with open('train_pos_merged.csv', 'w', newline='') as file:\n",
    "        # Write the modified data back to the CSV file\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data)\n",
    "\n",
    "first_row = [row for row in open('train_neg_merged.csv')][3]\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files\n",
    "df1 = pd.read_csv('train_neg_merged.csv')\n",
    "df2 = pd.read_csv('train_pos_merged.csv')\n",
    "\n",
    "# Combine the DataFrames\n",
    "combined_tweets = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_tweets.to_csv('train_data.csv', index=False)\n",
    "train  = pd.read_csv('train_data.csv')\n",
    "train[train['label'] == -1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/kaggle/input/twitter-dataset/test_data.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open('test.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['id', 'tweet'])  # Write the header row\n",
    "\n",
    "    for line in lines:\n",
    "        tweet_id, tweet_text = line.split(',', 1)\n",
    "        writer.writerow([tweet_id, tweet_text.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combi = train._append(test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"<user>\", \"@USER\")\n",
    "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"<url>\", \"HTTPURL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def normalizeToken(token):\n",
    "    lowercased_token = token.lower()\n",
    "    # if token.startswith(\"@\"):\n",
    "    #     return \"@USER\"\n",
    "    # elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n",
    "    #     return \"HTTPURL\"\n",
    "    if token == \"’\":\n",
    "        return \"'\"\n",
    "    elif token == \"…\":\n",
    "        return \"...\"\n",
    "    else:\n",
    "        return token\n",
    "\n",
    "\n",
    "def normalizeTweet(tweet):\n",
    "    tokens = tokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
    "    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n",
    "\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"cannot \", \"can not \")\n",
    "        .replace(\"n't \", \" n't \")\n",
    "        .replace(\"n 't \", \" n't \")\n",
    "        .replace(\"ca n't\", \"can't\")\n",
    "        .replace(\"ai n't\", \"ain't\")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"'m \", \" 'm \")\n",
    "        .replace(\"'re \", \" 're \")\n",
    "        .replace(\"'s \", \" 's \")\n",
    "        .replace(\"'ll \", \" 'll \")\n",
    "        .replace(\"'d \", \" 'd \")\n",
    "        .replace(\"'ve \", \" 've \")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\" p . m .\", \"  p.m.\")\n",
    "        .replace(\" p . m \", \" p.m \")\n",
    "        .replace(\" a . m .\", \" a.m.\")\n",
    "        .replace(\" a . m \", \" a.m \")\n",
    "    )\n",
    "\n",
    "    return \" \".join(normTweet.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizeTweet(combi)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
